{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amit-Prasad/transliteration/blob/main/transliteration_devnagri.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N-sIdYt5GI1",
        "outputId": "e28f7083-1a27-4417-e3b9-da962121db8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-12-06 13:06:14--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.194.207, 142.250.4.207, 172.253.118.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.194.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G  15.0MB/s    in 1m 50s  \n",
            "\n",
            "2023-12-06 13:08:05 (17.5 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoRtEVn8WAzb",
        "outputId": "0f8ae01c-fb05-47c5-e922-f1c2fa29155c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ],
      "source": [
        "!tar -xvf dakshina_dataset_v1.0.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGx4wbebWIR_",
        "outputId": "46c82efb-b8ff-46a3-b0ae-078d52f96c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "\n",
        "#device = xm.xla_device()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoiLbWuqWbr9"
      },
      "outputs": [],
      "source": [
        "data_features = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ns-meUTWfrK",
        "outputId": "56fc525e-5a30-4959-8a24-c8459aae7572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples: 44203\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 66\n",
            "Max sequence length for inputs: 20\n",
            "Max sequence length for outputs: 20\n"
          ]
        }
      ],
      "source": [
        "def prepareData(data_path):\n",
        "    # Vectorize the data.\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "    input_characters = set()\n",
        "    target_characters = set()\n",
        "    #with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = open(data_path,'rt', encoding='utf-8').\\\n",
        "            read().strip().split('\\n')\n",
        "    random.shuffle(lines)\n",
        "    for line in lines[:(len(lines) - 1)]:\n",
        "        target_text, input_text, _ = line.split(\"\\t\")\n",
        "        # We use \"tab\" as the \"start sequence\" character\n",
        "        # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "        target_text = target_text + \"\\n\"\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "        for char in input_text:\n",
        "            if char not in input_characters:\n",
        "                input_characters.add(char)\n",
        "        for char in target_text:\n",
        "            if char not in target_characters:\n",
        "                target_characters.add(char)\n",
        "\n",
        "    input_characters.add(\" \")\n",
        "    target_characters.add(\" \")\n",
        "    target_characters.add(\"\\t\")\n",
        "    input_characters = sorted(list(input_characters))\n",
        "    target_characters = sorted(list(target_characters))\n",
        "    num_encoder_tokens = len(input_characters)\n",
        "    num_decoder_tokens = len(target_characters)\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "    print(\"Number of samples:\", len(input_texts))\n",
        "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "    print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "    data_features.update({'input_token_index':input_token_index})\n",
        "    data_features.update({'target_token_index':target_token_index})\n",
        "    data_features.update({'num_encoder_tokens':num_encoder_tokens})\n",
        "    data_features.update({'num_decoder_tokens':num_decoder_tokens})\n",
        "    data_features.update({'max_encoder_seq_length':max_encoder_seq_length})\n",
        "    data_features.update({'max_decoder_seq_length':max_decoder_seq_length})\n",
        "\n",
        "\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length), dtype=\"int\"\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"int\"\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"int\"\n",
        "    )\n",
        "    mask = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length), dtype=\"bool\"\n",
        "    )\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t] = input_token_index[char]\n",
        "        encoder_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "        for t, char in enumerate(target_text):\n",
        "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "\n",
        "            decoder_target_data[i, t, target_token_index[char]] = 1 #one hot encoding\n",
        "            mask[i,t] = True\n",
        "            #if t > 0:\n",
        "                # decoder_target_data will be ahead by one timestep\n",
        "                # and will not include the start character.\n",
        "                #decoder_target_data[i, t - 1, target_token_index[char]] = 1\n",
        "        decoder_target_data[i, t + 1 :, target_token_index[\" \"]] = 1\n",
        "        mask[i,t+1:] = False\n",
        "        #decoder_target_data[i, t :, target_token_index[\" \"]] = 1\n",
        "\n",
        "    data_features.update({'encoder_input_data':encoder_input_data})\n",
        "    #data_features.update({'decoder_input_data':decoder_input_data})\n",
        "    data_features.update({'decoder_target_data':decoder_target_data})\n",
        "\n",
        "    data_features.update({'encoder_input_text_data':input_texts})\n",
        "    data_features.update({'decoder_target_text_data':target_texts})\n",
        "    data_features.update({'mask':mask})\n",
        "\n",
        "prepareData('dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pIaXXrtWkbK"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, batch_size, cell_type, n_layers, dropout):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        if cell_type == 'rnn':\n",
        "            self.encoder = nn.RNN(hidden_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n",
        "        elif cell_type == 'lstm':\n",
        "            self.encoder = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n",
        "        else:\n",
        "            self.encoder = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input)\n",
        "        output = embedded\n",
        "        output, hidden = self.encoder(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros(self.layers, batch_size, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmvuzEpmWol1"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, batch_size, cell_type, n_layers, dropout):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        if cell_type=='rnn':\n",
        "            self.decoder = nn.RNN(hidden_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n",
        "        elif cell_type=='lstm':\n",
        "            self.decoder = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n",
        "        else:\n",
        "            self.decoder = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "        #self.dropout = nn.Dropout(dropout2)\n",
        "\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        output = F.relu(output).view(-1, 1, self.hidden_size)\n",
        "        output, hidden = self.decoder(output, hidden)\n",
        "        #print(self.out(output).size())\n",
        "        #output = self.dropout(output)\n",
        "        output = self.softmax(self.out(output))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros(self.layers, batch_size, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xofPlzESf-UD"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, batch_size, cell_type, n_layers, max_length, dropout, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.cell_type=cell_type\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        if cell_type == 'rnn':\n",
        "            self.decoder = nn.RNN(self.hidden_size, self.hidden_size, n_layers, dropout = dropout, batch_first=True)\n",
        "        elif cell_type == 'lstm':\n",
        "            self.decoder = nn.LSTM(self.hidden_size, self.hidden_size, n_layers, dropout = dropout, batch_first=True)\n",
        "        else:\n",
        "            self.decoder = nn.GRU(self.hidden_size, self.hidden_size, n_layers, dropout = dropout, batch_first=True)\n",
        "\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input=input.view(-1,1)\n",
        "\n",
        "        embedded = self.embedding(input)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        if self.cell_type=='lstm':\n",
        "            attn_weights = F.softmax(\n",
        "                self.attn(torch.cat((embedded, hidden[0].view(-1,1, self.hidden_size)), 2)), dim=2)\n",
        "        else:\n",
        "            hidden = hidden.view(-1,1, self.hidden_size)\n",
        "            attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded, hidden), 2)), dim=2)\n",
        "        attn_applied = torch.bmm(attn_weights, encoder_outputs)\n",
        "\n",
        "        output = torch.cat((embedded, attn_applied), 2)\n",
        "        output = self.attn_combine(output)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        if self.cell_type=='lstm':\n",
        "            output, hidden = self.decoder(output, hidden)\n",
        "        else:\n",
        "            hidden = hidden.view(1,-1,self.hidden_size)\n",
        "            output, hidden = self.decoder(output, hidden)\n",
        "\n",
        "        output, hidden = self.decoder(output, hidden)\n",
        "\n",
        "        output = F.softmax(self.out(output), dim=2)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros(self.layers, batch_size, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5e_PGBNWsZo",
        "outputId": "a8e5791c-f25b-4b50-a55a-bdf74cb77274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([44203, 20])\n",
            "torch.Size([44203, 20, 66])\n"
          ]
        }
      ],
      "source": [
        "#change to tensors\n",
        "encoder_input_tensor = torch.from_numpy(data_features['encoder_input_data']).to(device)\n",
        "#decoder_input_tensor = torch.from_numpy(data_features['decoder_input_data']).to(device)\n",
        "decoder_target_tensor = torch.from_numpy(data_features['decoder_target_data']).to(device)\n",
        "mask = torch.from_numpy(data_features['mask']).to(device)\n",
        "print(encoder_input_tensor.size())\n",
        "#print(decoder_input_tensor.size())\n",
        "print(decoder_target_tensor.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAdpP-byWvLv"
      },
      "outputs": [],
      "source": [
        "def maskedCEloss(input, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(input, 1, target.view(-1, 1))).squeeze(1)\n",
        "    loss = crossEntropy.mean()\n",
        "\n",
        "\n",
        "    if math.isnan(loss) or math.isinf(loss):\n",
        "      print(f\"Nan found : {input}, {target}, {mask}\")\n",
        "\n",
        "    #loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a9dklsEIGlh"
      },
      "outputs": [],
      "source": [
        "def getOptimizer(name,enc_params,dec_params,lr):\n",
        "    if name == 'rmsprop':\n",
        "        enc_opt = optim.RMSprop(enc_params, lr, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
        "        dec_opt = optim.RMSprop(dec_params, lr, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
        "    elif name == 'adam' :\n",
        "        enc_opt = optim.Adam(enc_params, lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "        dec_opt = optim.Adam(dec_params, lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "    if name == 'sparseadam':\n",
        "        enc_opt = optim.SparseAdam(enc_params, lr, betas=(0.9, 0.999), eps=1e-08)\n",
        "        dec_opt = optim.SparseAdam(dec_params, lr, betas=(0.9, 0.999), eps=1e-08)\n",
        "\n",
        "    return (enc_opt,dec_opt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYDsAygIW0T4"
      },
      "outputs": [],
      "source": [
        "def train(input_tensor, target_tensor, masked, encoder, decoder, cell_type, batch_size, encoder_optimizer, decoder_optimizer, criterion, clip = 50.0,attention=0):\n",
        "    if cell_type=='lstm':\n",
        "        encoder_hidden = (encoder.initHidden(batch_size), encoder.initHidden(batch_size))\n",
        "    else:\n",
        "        encoder_hidden = encoder.initHidden(batch_size)\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor, encoder_hidden)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    #print(encoder_hidden)\n",
        "\n",
        "    loss=0\n",
        "\n",
        "    #With teacher forcing\n",
        "    decoder_input = torch.zeros_like(target_tensor[:,0,:]).to(device)\n",
        "    decoder_input[:,data_features['target_token_index']['\\t']] = 1\n",
        "    decoder_input = torch.argmax(decoder_input,1).view(-1,1)\n",
        "    for i in range(target_tensor.size()[1]):\n",
        "        if attention == 1:\n",
        "            #print(decoder_input.size())\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                torch.argmax(decoder_input,1).view(-1,1), decoder_hidden, encoder_output)\n",
        "\n",
        "        else:\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                    torch.argmax(decoder_input,1).view(-1,1), decoder_hidden)\n",
        "        #print(decoder_output.size())\n",
        "        if (masked[:,i].sum()!=0):\n",
        "            mask_loss, nTotal = maskedCEloss(decoder_output.squeeze().float(),torch.argmax(target_tensor[:,i,:],1),masked[:,i])\n",
        "            loss+=mask_loss\n",
        "\n",
        "        #loss+=criterion(decoder_output.squeeze().float(), torch.argmax(target_tensor[:,i,:],1))\n",
        "        decoder_input = target_tensor[:,i,:]\n",
        "\n",
        "    '''\n",
        "    #without teacher forcing\n",
        "    decoder_input = torch.zeros_like(target_tensor[:,0,:]).to(device)\n",
        "    decoder_input[:,data_features['target_token_index']['\\t']] = 1\n",
        "    decoder_input = torch.argmax(decoder_input,1).view(-1,1)\n",
        "    for i in range(target_tensor.size()[1]):\n",
        "        decoder_output, decoder_hidden = decoder(\n",
        "                    decoder_input, decoder_hidden)\n",
        "        topv, topi = decoder_output.topk(1)\n",
        "        if (masked[:,i].sum()!=0):\n",
        "            mask_loss, nTotal = maskedCEloss(decoder_output.squeeze().float(),torch.argmax(target_tensor[:,i,:],1),masked[:,i])\n",
        "            loss+=mask_loss\n",
        "        #print(mask_loss)\n",
        "        decoder_input = topi.squeeze().detach()\n",
        "    '''\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    if (clip > 1):\n",
        "        _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "        _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "\n",
        "    #xm.optimizer_step(encoder_optimizer, barrier=True)  # Note: Cloud TPU-specific code!\n",
        "    #xm.optimizer_step(decoder_optimizer, barrier=True)  # Note: Cloud TPU-specific code!\n",
        "    avg_loss = loss.item() / (input_tensor.size()[1]*batch_size)\n",
        "    if math.isnan(avg_loss) or math.isinf(avg_loss):\n",
        "      print(f'Loss num : {loss.item()} ; Loss Deno : {input_tensor.size()[1] , batch_size}')\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def trainIters(encoder, decoder, cell_type, n_epochs, batch_size, val_split,optimizer = 'rmsprop', learning_rate=0.001,clip = 0.0, attention=0):\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    encoder_optimizer, decoder_optimizer = getOptimizer(optimizer,encoder.parameters(),decoder.parameters(),learning_rate)\n",
        "\n",
        "    train_end_index = int(encoder_input_tensor.size()[0]*(1-val_split))\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "        total_loss=0\n",
        "        for j in range(0, train_end_index, batch_size):\n",
        "            input_tensor = encoder_input_tensor[j:j+batch_size]\n",
        "            #decoder_input = decoder_input_tensor[j:j+batch_size]\n",
        "            target_tensor = decoder_target_tensor[j:j+batch_size]\n",
        "            masked = mask[j:j+batch_size]\n",
        "            total_loss += train(input_tensor, target_tensor, masked, encoder,\n",
        "                     decoder, cell_type, batch_size , encoder_optimizer, decoder_optimizer, criterion, clip,attention)\n",
        "        avg_loss = float(total_loss)/(float(train_end_index)/batch_size)\n",
        "        if math.isnan(avg_loss) or math.isinf(avg_loss):\n",
        "            print({'incorrect_loss' : f'{total_loss},{train_end_index},{batch_size}'})\n",
        "        print(f'Epoch {i+1} ; Loss : {avg_loss}')\n",
        "    end=time.time()\n",
        "    print(\"Time Taken \"+str(end-start))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6mdXKhJW3Yg",
        "outputId": "f503d455-1322-4770-88ea-899b81cf300b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 ; Loss : 0.009588249565986754\n",
            "Epoch 2 ; Loss : 0.00275625240017066\n",
            "Epoch 3 ; Loss : 0.0018141970091602997\n",
            "Epoch 4 ; Loss : 0.0013127336684685775\n",
            "Epoch 5 ; Loss : 0.0009964150326097732\n",
            "Epoch 6 ; Loss : 0.0008069863688154085\n",
            "Epoch 7 ; Loss : 0.0006860249250157869\n",
            "Epoch 8 ; Loss : 0.0006095264425186489\n",
            "Epoch 9 ; Loss : 0.0005659350665106111\n",
            "Epoch 10 ; Loss : 0.0005165679206028765\n",
            "Epoch 11 ; Loss : 0.00047878336147703243\n",
            "Epoch 12 ; Loss : 0.0004448100369753065\n",
            "Epoch 13 ; Loss : 0.00042970656191584705\n",
            "Epoch 14 ; Loss : 0.0004027592843331683\n",
            "Epoch 15 ; Loss : 0.0003853243688999528\n",
            "Epoch 16 ; Loss : 0.00035514701855445706\n",
            "Epoch 17 ; Loss : 0.0003440992332754091\n",
            "Epoch 18 ; Loss : 0.0003375482949656328\n",
            "Epoch 19 ; Loss : 0.0003279341196078629\n",
            "Epoch 20 ; Loss : 0.00032041225327466004\n",
            "Time Taken 1284.7300944328308\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 1024\n",
        "batch_size = 64\n",
        "encoder1 = EncoderRNN(data_features['num_encoder_tokens'], hidden_size, batch_size, 'lstm', 2, 0.1).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, data_features['num_decoder_tokens'], batch_size, 'lstm', 2, 0.1).to(device)\n",
        "#attn_decoder1 = AttnDecoderRNN(hidden_size, data_features['num_decoder_tokens'], batch_size, 'lstm', 1, data_features['max_encoder_seq_length'], dropout_p=0.1, dropout=0).to(device)\n",
        "trainIters(encoder1, decoder1, 'lstm', 20, batch_size, 0.1, optimizer = 'adam', learning_rate=0.001, clip=3000,attention=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_pMXUeFGW7b9"
      },
      "outputs": [],
      "source": [
        "def tensorPairs(word1, word2):\n",
        "    tensor1 = torch.zeros([len(word1), 1], dtype=torch.long, device=device)\n",
        "    for i, char in enumerate(word1):\n",
        "        tensor1[i] = data_features['input_token_index'][char]\n",
        "\n",
        "    tensor2 = torch.zeros([len(word2), 1], dtype=torch.long, device=device)\n",
        "    for i, char in enumerate(word2):\n",
        "        tensor2[i] = data_features['target_token_index'][char]\n",
        "\n",
        "    return (tensor1, tensor2)\n",
        "\n",
        "\n",
        "def get_key(d, val):\n",
        "    return [k for k, v in d.items() if v == val]\n",
        "\n",
        "def WordTensor(tensor):\n",
        "    char = get_key(data_features['target_token_index'], tensor)\n",
        "    #print(char)\n",
        "    return char\n",
        "\n",
        "\n",
        "def evaluate(encoder, decoder, cell_type, input_tensor, max_length=data_features['max_encoder_seq_length'], attention=1):\n",
        "    with torch.no_grad():\n",
        "        #print(word)\n",
        "        #input_tensor = tensorWord(word)\n",
        "        input_length = input_tensor.size()[0]\n",
        "\n",
        "        if cell_type=='lstm':\n",
        "            encoder_hidden = (encoder.initHidden(1), encoder.initHidden(1))\n",
        "        else:\n",
        "            encoder_hidden = encoder.initHidden(1)\n",
        "\n",
        "        encoder_outputs = torch.zeros(1, max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei].view(1,-1),\n",
        "                                                     encoder_hidden)\n",
        "\n",
        "            encoder_outputs[0,ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[data_features['target_token_index']['\\t']]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_word = ''\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            if attention == 1:\n",
        "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                    decoder_input, decoder_hidden, encoder_outputs)\n",
        "            else:\n",
        "                decoder_output, decoder_hidden = decoder(\n",
        "                    decoder_input, decoder_hidden)\n",
        "            #decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if chr(ord(WordTensor(topi.item())[0])) == '\\n':\n",
        "                #decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                #print(WordTensor(topi.item()))\n",
        "                decoded_word = decoded_word+chr(ord(WordTensor(topi.item())[0]))\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_word#, decoder_attentions[:di + 1]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "F0CDKb2aeqOn"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, cell_type,  n=10,attention=0):\n",
        "    for i in range(n):\n",
        "        #index = random.randint(0, len(data_features['encoder_input_data'])-1)\n",
        "        index = i\n",
        "        pair = tensorPairs(data_features['encoder_input_text_data'][index], data_features['decoder_target_text_data'][index])\n",
        "        print('>', data_features['encoder_input_text_data'][index])\n",
        "        print('=', data_features['decoder_target_text_data'][index][0:-1])\n",
        "        output_word = evaluate(encoder, decoder, cell_type,  pair[0],attention=attention)\n",
        "        print('<', output_word)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7ZRpTN4Jes0D"
      },
      "outputs": [],
      "source": [
        "def validate(encoder, decoder, cell_type, val_split, attention=0):\n",
        "    val_start_index = int(encoder_input_tensor.size()[0]*(1-val_split))+1\n",
        "    accuracy=0\n",
        "    for index in range(val_start_index, len(data_features['encoder_input_text_data'])):\n",
        "        pair = tensorPairs(data_features['encoder_input_text_data'][index], data_features['decoder_target_text_data'][index])\n",
        "        output_word = evaluate(encoder, decoder, cell_type,  pair[0],attention=attention)\n",
        "\n",
        "        if (output_word == data_features['decoder_target_text_data'][index][0:-1]):\n",
        "            accuracy+=1\n",
        "    print(accuracy/(len(data_features['encoder_input_text_data'])-val_start_index))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2EVKPZ6mHk9g",
        "outputId": "ba342ef2-64f7-4412-c392-3a2eb6abca37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "antah  अंतः\n",
            "antrmukh  अंतर्मुख\n",
            "agvaai  अगवाई\n",
            "achyut  अच्युत\n",
            "agyaat  अज्ञात\n",
            "agyat  अज्ञात\n",
            "atakta  अटकता\n",
            "atkata  अटकता\n",
            "adhivas  अधिवास\n",
            "adhiwas  अधिवास\n",
            "apnani  अपनानी\n",
            "africa  अफ्रीका\n",
            "amaanat  अमानत\n",
            "amavasya  अमावस्या\n",
            "alankaaron  अलंकारों\n",
            "avarodhak  अवरोधक\n",
            "avrodhak  अवरोधक\n",
            "avasthaayen  अवस्थाएं\n",
            "avsthaen  अवस्थाएं\n",
            "avsthayen  अवस्थाएं\n",
            "awasthaen  अवस्थाएं\n",
            "asmanye  असमान्य\n",
            "asurakshit  असुरक्षित\n",
            "item  आइटम\n",
            "aakaash  आकाश\n",
            "aakash  आकाश\n",
            "aake  आके\n",
            "aagat  आगत\n",
            "aagaman  आगमन\n",
            "aagman  आगमन\n",
            "aagashe  आगाशे\n",
            "aazmaanaa  आजमाना\n",
            "aadaan  आदान\n",
            "aadhaarit  आधारित\n",
            "aadharit  आधारित\n",
            "aadheen  आधीन\n",
            "aadhunik  आधुनिक\n",
            "aapka  आपका\n",
            "aapkaa  आपका\n",
            "aapadaaaen  आपदाएं\n",
            "aapdaein  आपदाएं\n",
            "aapdayein  आपदाएं\n",
            "aamdan  आमदन\n",
            "aamdani  आमदनी\n",
            "aamdi  आमदी\n",
            "aayojak  आयोजक\n",
            "aaraam  आराम\n",
            "aarushi  आरुषि\n",
            "aarya  आर्य\n",
            "arya  आर्य\n",
            "aaryika  आर्यिका\n",
            "aryon  आर्यों\n",
            "aalsi  आलसी\n",
            "aalochak  आलोचक\n",
            "aalochakon  आलोचकों\n",
            "alochakon  आलोचकों\n",
            "aalha  आल्हा\n",
            "aalhaa  आल्हा\n",
            "awaazon  आवाज़ों\n",
            "aavishakaar  आविष्कार\n",
            "aavishkaar  आविष्कार\n",
            "aavritti  आवृत्ति\n",
            "aashanka  आशंका\n",
            "aastik  आस्तिक\n",
            "ilayichi  इलाइची\n",
            "ilaaj  इलाज\n",
            "ilaj  इलाज\n",
            "ishaak  इशाक\n",
            "ishak  इशाक\n",
            "ishaaraa  इशारा\n",
            "isaaiyon  ईसाइयों\n",
            "isaiyon  ईसाइयों\n",
            "uttara  उत्तरा\n",
            "uttra  उत्तरा\n",
            "udaa  उदा\n",
            "upneta  उपनेता\n",
            "uplabdh  उपलब्ध\n",
            "ubaal  उबाल\n",
            "umanath  उमानाथ\n",
            "usak  उसक\n",
            "usaki  उसकी\n",
            "rushikul  ऋषिकुल\n",
            "eksaath  एकसाथ\n",
            "eksath  एकसाथ\n",
            "eliminator  एलिमिनेटर\n",
            "ainth  ऐंठ\n",
            "enth  ऐंठ\n",
            "ainthey  ऐंठे\n",
            "aise  ऐसे\n",
            "olavrishti  ओलावृष्टि\n",
            "owem  ओवम\n",
            "kankad  कंकड़\n",
            "kankar  कंकर\n",
            "kambalon  कंबलों\n",
            "kanblon  कंबलों\n",
            "consultant  कंसलटेंट\n",
            "kacche  कच्चे\n",
            "kanaal  कनाल\n",
            "kanal  कनाल\n",
            "kanishthika  कनिष्ठिका\n",
            "kanishthikaa  कनिष्ठिका\n",
            "kanyaein  कन्याएं\n",
            "kapdo  कपड़ो\n",
            "kaping  कपिंग\n",
            "kping  कपिंग\n",
            "kabron  कब्रों\n",
            "kamtar  कमतर\n",
            "kamaauu  कमाऊ\n",
            "kamaate  कमाते\n",
            "karam  करम\n",
            "karoor  करूर\n",
            "kalraj  कलराज\n",
            "kalpanaon  कल्पनाओं\n",
            "kalpnaon  कल्पनाओं\n",
            "kashton  कष्टों\n",
            "kasarat  कसरत\n",
            "kasrat  कसरत\n",
            "kahne  कहने\n",
            "kahaniyon  कहानियों\n",
            "kanji  कांजी\n",
            "kakoo  काकू\n",
            "kaagaj  कागज़\n",
            "kaagaz  कागज़\n",
            "kagaz  कागज़\n",
            "kaaji  काजी\n",
            "kaji  काजी\n",
            "kazi  काजी\n",
            "qazi  काजी\n",
            "kajoo  काजू\n",
            "kalika  कालिका\n",
            "kalighat  कालीघाट\n",
            "kaav  काव\n",
            "kasim  कासिम\n",
            "qasim  कासिम\n",
            "kiya  किया\n",
            "killat  किल्लत\n",
            "kinote  कीनोट\n",
            "kuposhit  कुपोषित\n",
            "kumau  कुमाऊ\n",
            "kural  कुराल\n",
            "kursiyon  कुर्सियों\n",
            "kush  कुश\n",
            "koot  कूट\n",
            "kut  कूट\n",
            "candidate  कैंडिडेट\n",
            "catlin  कैटलिन\n",
            "koling  कोलिंग\n",
            "kaushal  कौशल\n",
            "kaushalata  कौशलता\n",
            "kaushalta  कौशलता\n",
            "krantiyon  क्रांतियों\n",
            "cric  क्रिक\n",
            "crick  क्रिक\n",
            "chrome  क्रोम\n",
            "clay  क्ले\n",
            "queens  क्वींस\n",
            "kshitij  क्षितिज\n",
            "kshetron  क्षेत्रों\n",
            "shetron  क्षेत्रों\n",
            "khajraana  खजराना\n",
            "khajrana  खजराना\n",
            "khatakati  खटकती\n",
            "khatakti  खटकती\n",
            "khat  खत\n",
            "khariya  खरिया\n",
            "khaen  खाएं\n",
            "khayen  खाएं\n",
            "khaate  खाते\n",
            "khaatey  खाते\n",
            "khinchkar  खींचकर\n",
            "khushiyan  खुशियां\n",
            "khaini  खैनी\n",
            "khoya  खोया\n",
            "gangrel  गंगरेल\n",
            "gangajal  गंगाजल\n",
            "gandh  गंध\n",
            "gandha  गंध\n",
            "gatisheel  गतिशील\n",
            "gatishil  गतिशील\n",
            "galgand  गलगंड\n",
            "gajipur  गाजीपुर\n",
            "gayatri  गायत्री\n",
            "ginte  गिनते\n",
            "girjaghar  गिरजाघर\n",
            "girjagharon  गिरजाघरों\n",
            "gudna  गुदना\n",
            "gumaan  गुमान\n",
            "grihnagar  गृहनगर\n",
            "gochar  गोचर\n",
            "gopichand  गोपीचंद\n",
            "gaumukh  गौमुख\n",
            "greest  ग्रीस्ट\n",
            "ghataayaa  घटाया\n",
            "ghanakar  घनाकार\n",
            "ghisai  घिसाई\n",
            "ghumakkdon  घुमक्कड़ों\n",
            "ghoonsa  घूंसा\n",
            "ghongha  घोंघा\n",
            "ghontane  घोंटने\n",
            "ghontne  घोंटने\n",
            "ghot  घोट\n",
            "ghodi  घोड़ी\n",
            "ghost  घोस्ट\n",
            "changul  चंगुल\n",
            "chand  चंद\n",
            "chandan  चंदन\n",
            "chandon  चंदों\n",
            "chandrabali  चंद्रबली\n",
            "chakkar  चक्कर\n",
            "chakshu  चक्षु\n",
            "chadhane  चढ़ने\n",
            "chadhne  चढ़ने\n",
            "charit  चरित\n",
            "churcha  चर्चा\n",
            "charchaon  चर्चाओं\n",
            "chachaji  चाचाजी\n",
            "chadar  चादर\n",
            "chaalit  चालित\n",
            "chikitsak  चिकित्सक\n",
            "chikitsakon  चिकित्सकों\n",
            "chitrkar  चित्रकार\n",
            "chipakane  चिपकने\n",
            "chipakne  चिपकने\n",
            "chimati  चिमटी\n",
            "chimti  चिमटी\n",
            "chi  ची\n",
            "chune  चुने\n",
            "chero  चेरो\n",
            "chest  चेस्ट\n",
            "chor  चोर\n",
            "choron  चोरों\n",
            "chauk  चौक\n",
            "chowk  चौक\n",
            "chauka  चौका\n",
            "chaukiyan  चौकियां\n",
            "chauki  चौकी\n",
            "chauguni  चौगुनी\n",
            "chausinga  चौसिंगा\n",
            "chhavi  छवी\n",
            "chhaati  छाती\n",
            "chheenk  छींक\n",
            "chhink  छींक\n",
            "chhinke  छींके\n",
            "chhupkar  छुपकर\n",
            "chhuptaa  छुपता\n",
            "chhura  छुरा\n",
            "chhuraa  छुरा\n",
            "chhure  छुरे\n",
            "chhootane  छूटने\n",
            "jagayega  जगाएगा\n",
            "jgaayega  जगाएगा\n",
            "jgayegaa  जगाएगा\n",
            "jadein  जड़ें\n",
            "jaden  जड़ें\n",
            "jatra  जतरा\n",
            "janahaani  जनहानि\n",
            "janahani  जनहानि\n",
            "janhaani  जनहानि\n",
            "jamaate  जमाती\n",
            "jamaati  जमाती\n",
            "jamayaa  जमाया\n",
            "jay  जय\n",
            "jalte  जलते\n",
            "jalesar  जलेसर\n",
            "jassi  जस्सी\n",
            "jaat  जात\n",
            "jaatak  जातक\n",
            "jatak  जातक\n",
            "jaatakon  जातकों\n",
            "jaatkon  जातकों\n",
            "jaatiyon  जातियों\n",
            "jatiyon  जातियों\n",
            "jaawar  जावर\n",
            "jing  जिंग\n",
            "jiya  जिया\n",
            "jeen  जीन\n",
            "jeenon  जीनों\n",
            "jinon  जीनों\n",
            "jipani  जीपनी\n",
            "jeeyo  जीयो\n",
            "jeera  जीरा\n",
            "jira  जीरा\n",
            "jeevant  जीवंत\n",
            "jivant  जीवंत\n",
            "jeevan  जीवन\n",
            "jeewan  जीवन\n",
            "jivan  जीवन\n",
            "jiwan  जीवन\n",
            "jeevniyaan  जीवनियां\n",
            "jivaniyaan  जीवनियां\n",
            "jivaniyan  जीवनियां\n",
            "judaai  जुदाई\n",
            "juber  जुबेर\n",
            "juraab  जुराब\n",
            "jurab  जुराब\n",
            "julaahon  जुलाहों\n",
            "julahon  जुलाहों\n",
            "zulmon  जुल्मों\n",
            "junior  जूनियर\n",
            "jetha  जेठा\n",
            "jenni  जेनी\n",
            "joddta  जोड़ता\n",
            "jodiyon  जोड़ियों\n",
            "jodo  जोड़ो\n",
            "gyatavya  ज्ञातव्य\n",
            "gyapit  ज्ञापित\n",
            "jvaaron  ज्वारों\n",
            "jvaron  ज्वारों\n",
            "jh  झ\n",
            "jhalakti  झलकती\n",
            "jhalkati  झलकती\n",
            "jheel  झील\n",
            "jheelon  झीलों\n",
            "jhilon  झीलों\n",
            "jhuthala  झुठला\n",
            "jhuthla  झुठला\n",
            "takara  टकरा\n",
            "takra  टकरा\n",
            "teekon  टीकों\n",
            "tennis  टेनिस\n",
            "testing  टेस्टिंग\n",
            "tonga  टोंगा\n",
            "toka  टोका\n",
            "tuning  ट्यूनिंग\n",
            "trabal  ट्रेबल\n",
            "trebal  ट्रेबल\n",
            "trot  ट्रॉट\n",
            "trott  ट्रॉट\n",
            "thikane  ठिकाने\n",
            "theinga  ठेंगा\n",
            "thenga  ठेंगा\n",
            "thengaa  ठेंगा\n",
            "domenic  डोमेनिक\n",
            "dolane  डोलने\n",
            "dhale  ढले\n",
            "tantrikao  तंत्रिकाओं\n",
            "tantrikaon  तंत्रिकाओं\n",
            "tani  तनी\n",
            "tarfa  तरफा\n",
            "taras  तरस\n",
            "tarashe  तराशे\n",
            "tarkvaad  तर्कवाद\n",
            "tarkvad  तर्कवाद\n",
            "tarkwaad  तर्कवाद\n",
            "tarkon  तर्कों\n",
            "talawe  तलवे\n",
            "talve  तलवे\n",
            "talawon  तलवों\n",
            "talvon  तलवों\n",
            "taantaa  तांता\n",
            "taapmaanon  तापमानों\n",
            "tirupati  तिरूपति\n",
            "tiwari  तिवारी\n",
            "teeron  तीरों\n",
            "tejaji  तेजाजी\n",
            "teji  तेजी\n",
            "triya  त्रिया\n",
            "dampatiyon  दंपतियों\n",
            "danptiyon  दंपतियों\n",
            "darjanon  दर्जनों\n",
            "darp  दर्प\n",
            "dali  दली\n",
            "daataa  दाता\n",
            "daabholkar  दाभोलकर\n",
            "daah  दाह\n",
            "dikhlaa  दिखला\n",
            "dikhaai  दिखाई\n",
            "dikhai  दिखाई\n",
            "dikhayi  दिखायी\n",
            "dita  दिता\n",
            "ditaa  दिता\n",
            "divya  दिव्य\n",
            "divyata  दिव्यता\n",
            "deepan  दीपन\n",
            "dipan  दीपन\n",
            "deepika  दीपिका\n",
            "dipika  दीपिका\n",
            "dukaanon  दुकानों\n",
            "dukhiyon  दुखियों\n",
            "dukhon  दुखों\n",
            "dubhashiyon  दुभाषियों\n",
            "durgun  दुर्गुण\n",
            "durghatna  दुर्घटना\n",
            "durghtna  दुर्घटना\n",
            "duvidhaaen  दुविधाएं\n",
            "duvidhaayein  दुविधाएं\n",
            "duvidhaen  दुविधाएं\n",
            "doordarshan  दूरदर्शन\n",
            "devnaar  देवनार\n",
            "devaapur  देवापुर\n",
            "devapur  देवापुर\n",
            "devi  देवी\n",
            "deshkaal  देशकाल\n",
            "deshkal  देशकाल\n",
            "deshke  देशके\n",
            "desai  देसाई\n",
            "dvadash  द्वादश\n",
            "dhandhe  धंधे\n",
            "dhamkaataa  धमकाता\n",
            "dharmnath  धर्मनाथ\n",
            "dharmesh  धर्मेश\n",
            "dhaak  धाक\n",
            "dhaaga  धागा\n",
            "dhaagaa  धागा\n",
            "dhaatuein  धातुएं\n",
            "dhaatuyein  धातुएं\n",
            "dhaarak  धारक\n",
            "dharak  धारक\n",
            "dhaari  धारी\n",
            "dhari  धारी\n",
            "dhunaai  धुनाई\n",
            "dhunai  धुनाई\n",
            "dhuriya  धुरिया\n",
            "dhuriyaa  धुरिया\n",
            "dhuni  धूनी\n",
            "dhairyavaan  धैर्यवान\n",
            "dhokhebaaj  धोखेबाज\n",
            "dhona  धोना\n",
            "dhonaa  धोना\n",
            "dhoraaji  धोराजी\n",
            "dhoraji  धोराजी\n",
            "dhoron  धोरों\n",
            "nagine  नगीने\n",
            "najim  नजीम\n",
            "namkeen  नमकीन\n",
            "narsanhar  नरसंहार\n",
            "narsanharon  नरसंहारों\n",
            "navketan  नवकेतन\n",
            "nava  नवा\n",
            "navaa  नवा\n",
            "nawanagar  नवानगर\n",
            "navya  नव्या\n",
            "naseem  नसीम\n",
            "naakaam  नाकाम\n",
            "napane  नापने\n",
            "naam  नाम\n",
            "naamdhari  नामधारी\n",
            "naamo  नामो\n",
            "namo  नामो\n",
            "nayak  नायक\n",
            "naraji  नाराजी\n",
            "naasar  नासर\n",
            "nikaalani  निकालनी\n",
            "niki  निकी\n",
            "niketan  निकेतन\n",
            "nikhar  निखर\n",
            "nikhari  निखरी\n",
            "nitiyon  नितियों\n",
            "nityaa  नित्या\n",
            "nidhaan  निधान\n",
            "nidhan  निधान\n",
            "nipatara  निपटारा\n",
            "niptaara  निपटारा\n",
            "niptaaraa  निपटारा\n",
            "niyantaa  नियंता\n",
            "niya  निया\n",
            "niyaa  निया\n",
            "nirdhaarit  निर्धारित\n",
            "nirdharit  निर्धारित\n",
            "nirbaadh  निर्बाध\n",
            "nihlaani  निहलानी\n",
            "nihaalchand  निहालचंद\n",
            "nihalchand  निहालचंद\n",
            "neetiyon  नीतियों\n",
            "neelaabh  नीलाभ\n",
            "noon  नून\n",
            "noobiyaa  नूबिया\n",
            "nrityon  नृत्यों\n",
            "nrutyon  नृत्यों\n",
            "neco  नेको\n",
            "neko  नेको\n",
            "nectar  नेक्टर\n",
            "neg  नेग\n",
            "nemo  नेमो\n",
            "nelsan  नेल्सन\n",
            "nelson  नेल्सन\n",
            "nations  नेशंस\n",
            "naino  नैनो\n",
            "notes  नोट्स\n",
            "punjab  पंजाब\n",
            "pakaayein  पकाएं\n",
            "pakaayen  पकाएं\n",
            "pakayen  पकाएं\n",
            "pakshapat  पक्षपात\n",
            "pakshpaat  पक्षपात\n",
            "pagalaa  पगला\n",
            "pagla  पगला\n",
            "pachaataa  पचाता\n",
            "pachata  पचाता\n",
            "patakkar  पटककर\n",
            "patkathaayein  पटकथाएं\n",
            "patkathaein  पटकथाएं\n",
            "patkathayen  पटकथाएं\n",
            "patki  पटकी\n",
            "patavaari  पटवारी\n",
            "patwari  पटवारी\n",
            "patiya  पटिया\n",
            "patiyaa  पटिया\n",
            "padega  पड़ेगा\n",
            "padegaa  पड़ेगा\n",
            "pummi  पम्मी\n",
            "pardesi  परदेसी\n",
            "parva  परवा\n",
            "parvaa  परवा\n",
            "parwa  परवा\n",
            "parwaa  परवा\n",
            "paraadheen  पराधीन\n",
            "paraadhin  पराधीन\n",
            "paradheen  पराधीन\n",
            "paradhin  पराधीन\n",
            "parikshaen  परीक्षाएं\n",
            "parosane  परोसने\n",
            "palatne  पलटने\n",
            "paltane  पलटने\n",
            "palaari  पलारी\n",
            "palari  पलारी\n",
            "plus  पल्स\n",
            "pulse  पल्स\n",
            "pahan  पहन\n",
            "pahantaa  पहनता\n",
            "pahanate  पहनते\n",
            "pahante  पहनते\n",
            "pehnate  पहनते\n",
            "pahnaati  पहनाती\n",
            "pahani  पहनी\n",
            "pahni  पहनी\n",
            "pehni  पहनी\n",
            "pahla  पहला\n",
            "pahlaa  पहला\n",
            "pehla  पहला\n",
            "paheliyaan  पहेलियां\n",
            "paheliyan  पहेलियां\n",
            "paheliyon  पहेलियों\n",
            "paathshaalaayein  पाठशालाएं\n",
            "pathshalayein  पाठशालाएं\n",
            "paathyakram  पाठ्यक्रम\n",
            "pathyakram  पाठ्यक्रम\n",
            "pathykram  पाठ्यक्रम\n",
            "pathykramon  पाठ्यक्रमों\n",
            "pinta  पिंटा\n",
            "pic  पिक\n",
            "pick  पिक\n",
            "pik  पिक\n",
            "picture  पिक्चर\n",
            "pihaani  पिहानी\n",
            "pitne  पीटने\n",
            "pedia  पीडिया\n",
            "pidiya  पीडिया\n",
            "pidiyaa  पीडिया\n",
            "pilepan  पीलेपन\n",
            "putliyon  पुतलियों\n",
            "putali  पुतली\n",
            "punargathan  पुनर्गठन\n",
            "puraane  पुराने\n",
            "purane  पुराने\n",
            "purjon  पुर्जों\n",
            "purzon  पुर्जों\n",
            "pushp  पुष्प\n",
            "pushpa  पुष्प\n",
            "pushpit  पुष्पित\n",
            "pushpon  पुष्पों\n",
            "poonjikaran  पूंजीकरण\n",
            "poojaapaath  पूजापाठ\n",
            "purvaabhaas  पूर्वाभास\n",
            "purvaabhas  पूर्वाभास\n",
            "purvabhaas  पूर्वाभास\n",
            "purvabhas  पूर्वाभास\n",
            "pes  पेस\n",
            "paim  पैम\n",
            "pokar  पोकर\n",
            "poker  पोकर\n",
            "pyade  प्यादे\n",
            "prakriya  प्रक्रिया\n",
            "prakshepak  प्रक्षेपक\n",
            "pratibhutiyan  प्रतिभूतियां\n",
            "pratiyon  प्रतियों\n",
            "prativaad  प्रतिवाद\n",
            "prativad  प्रतिवाद\n",
            "prativadi  प्रतिवादी\n",
            "prapaat  प्रपात\n",
            "prapat  प्रपात\n",
            "prabodhini  प्रबोधिनी\n",
            "pravatti  प्रवत्ति\n",
            "prashaasanik  प्रशासनिक\n",
            "prashasanik  प्रशासनिक\n",
            "prashikshu  प्रशिक्षु\n",
            "prastaavon  प्रस्तावों\n",
            "prahsan  प्रहसन\n",
            "prahaarak  प्रहारक\n",
            "praan  प्राण\n",
            "prathamik  प्राथमिक\n",
            "prathmik  प्राथमिक\n",
            "prathmikta  प्राथमिकता\n",
            "pradhyapakon  प्राध्यापकों\n",
            "prapt  प्राप्त\n",
            "prekshakon  प्रेक्षकों\n",
            "premlata  प्रेमलता\n",
            "prema  प्रेमा\n",
            "premi  प्रेमी\n",
            "programers  प्रोग्रामर्स\n",
            "proto  प्रोटो\n",
            "funda  फंडा\n",
            "funding  फंडिंग\n",
            "fatane  फटने\n",
            "phatane  फटने\n",
            "phatne  फटने\n",
            "fatehgadh  फतेहगढ़\n",
            "pharase  फरसे\n",
            "farheen  फरहीन\n",
            "farhin  फरहीन\n",
            "ferth  फर्थ\n",
            "falta  फलता\n",
            "phalata  फलता\n",
            "falne  फलने\n",
            "phalne  फलने\n",
            "falibhoot  फलीभूत\n",
            "falibhut  फलीभूत\n",
            "phalibhoot  फलीभूत\n",
            "faansi  फांसी\n",
            "phaaph  फाफ\n",
            "finger  फिंगर\n",
            "fislaa  फिसला\n",
            "fulaaye  फुलाए\n",
            "fulae  फुलाए\n",
            "fefda  फेफड़ा\n",
            "fact  फैक्ट\n",
            "failaaiye  फैलाइए\n",
            "foda  फोड़ा\n",
            "fodaa  फोड़ा\n",
            "phoda  फोड़ा\n",
            "fodon  फोड़ों\n",
            "franciska  फ्रांसिस्का\n",
            "flin  फ्लिन\n",
            "flesh  फ्लेश\n",
            "bang  बंग\n",
            "bung  बंग\n",
            "bangash  बंगश\n",
            "bangsh  बंगश\n",
            "bantne  बंटने\n",
            "bandhuon  बंधुओं\n",
            "bakayon  बकायों\n",
            "bageechi  बगीची\n",
            "bagichi  बगीची\n",
            "badhaati  बढ़ाती\n",
            "batlaae  बतलाए\n",
            "batlaaye  बतलाए\n",
            "batlae  बतलाए\n",
            "batlaye  बतलाए\n",
            "badali  बदली\n",
            "badli  बदली\n",
            "banaam  बनाम\n",
            "banam  बनाम\n",
            "baniya  बनिया\n",
            "barsa  बरसा\n",
            "barsaata  बरसाता\n",
            "barhad  बरहद\n",
            "ballabh  बल्लभ\n",
            "bastiyon  बस्तियों\n",
            "bahroopiya  बहरूपिया\n",
            "bahaav  बहाव\n",
            "bahavalpur  बहावलपुर\n",
            "bahawalpur  बहावलपुर\n",
            "baag  बाग\n",
            "bag  बाग\n",
            "bagali  बागली\n",
            "baagiyon  बागियों\n",
            "baago  बागो\n",
            "bago  बागो\n",
            "badha  बाधा\n",
            "baadhaaon  बाधाओं\n",
            "baadhaon  बाधाओं\n",
            "badhaaon  बाधाओं\n",
            "badhaon  बाधाओं\n",
            "baadhit  बाधित\n",
            "baani  बानी\n",
            "bani  बानी\n",
            "bapaa  बापा\n",
            "baabat  बाबत\n",
            "babat  बाबत\n",
            "barah  बारह\n",
            "barahsinga  बारहसिंगा\n",
            "bavaji  बावजी\n",
            "baawaraa  बावरा\n",
            "bisva  बिस्वा\n",
            "biswa  बिस्वा\n",
            "beejak  बीजक\n",
            "bijak  बीजक\n",
            "beem  बीम\n",
            "beemon  बीमों\n",
            "bimon  बीमों\n",
            "beerganj  बीरगंज\n",
            "bule  बुले\n",
            "bulletin  बुलेटिन\n",
            "bechkar  बेचकर\n",
            "bechate  बेचते\n",
            "bechte  बेचते\n",
            "benis  बेनिस\n",
            "belacha  बेलचा\n",
            "basin  बेसिन\n",
            "besin  बेसिन\n",
            "baithate  बैठते\n",
            "baini  बैनी\n",
            "ballistic  बैलिस्टिक\n",
            "bodara  बोदरा\n",
            "bona  बोना\n",
            "born  बोर्न\n",
            "bolane  बोलने\n",
            "bauni  बौनी\n",
            "brajesh  ब्रजेश\n",
            "brijesh  ब्रजेश\n",
            "brigatn  ब्रिगटन\n",
            "brigton  ब्रिगटन\n",
            "brok  ब्रोक\n",
            "bhagaane  भगाने\n",
            "bhagane  भगाने\n",
            "bhadakaaya  भड़काया\n",
            "bhadakaya  भड़काया\n",
            "bhadkaayaa  भड़काया\n",
            "bhadkaya  भड़काया\n",
            "bhabhut  भभूत\n",
            "bhay  भय\n",
            "bharte  भरते\n",
            "bhaate  भाते\n",
            "bhavnaon  भावनाओं\n",
            "bheed  भीड़\n",
            "bheera  भीरा\n",
            "bhira  भीरा\n",
            "bheesham  भीष्म\n",
            "bhugata  भुगता\n",
            "bhugta  भुगता\n",
            "bhookh  भूख\n",
            "bhukh  भूख\n",
            "bhookhi  भूखी\n",
            "bhukhi  भूखी\n",
            "bhoogol  भूगोल\n",
            "bhugol  भूगोल\n",
            "bhootal  भूतल\n",
            "bhula  भूला\n",
            "bhooli  भूली\n",
            "bhulo  भूलो\n",
            "bhulon  भूलों\n",
            "bhusi  भूसी\n",
            "bhusvami  भूस्वामी\n",
            "bhuswami  भूस्वामी\n",
            "bhogatey  भोगते\n",
            "bhogte  भोगते\n",
            "bholi  भोली\n",
            "bhaunho  भौंहों\n",
            "bhaunhon  भौंहों\n",
            "bhraant  भ्रांत\n",
            "bhraantiyan  भ्रांतियां\n",
            "bhrantiyan  भ्रांतियां\n",
            "bhraantiyon  भ्रांतियों\n",
            "bhrantiyon  भ्रांतियों\n",
            "manch  मंच\n",
            "manjil  मंजिल\n",
            "manzil  मंजिल\n",
            "manjilein  मंजिलें\n",
            "manjilen  मंजिलें\n",
            "manzilein  मंजिलें\n",
            "mantriyon  मंत्रियों\n",
            "machaaye  मचाए\n",
            "machaye  मचाए\n",
            "machaanon  मचानों\n",
            "matpatron  मतपत्रों\n",
            "madhupur  मधुपुर\n",
            "maryaadaa  मर्यादा\n",
            "maryaadaaon  मर्यादाओं\n",
            "masaaledaar  मसालेदार\n",
            "masoor  मसूर\n",
            "masur  मसूर\n",
            "mahlon  महलों\n",
            "mahanatam  महानतम\n",
            "mahanideshak  महानिदेशक\n",
            "mhanideshak  महानिदेशक\n",
            "mahamantra  महामंत्र\n",
            "mhamantra  महामंत्र\n",
            "mahavigyan  महाविज्ञान\n",
            "mahavidya  महाविद्या\n",
            "mahasingh  महासिंह\n",
            "mahasinh  महासिंह\n",
            "maang  मांग\n",
            "maangi  मांगी\n",
            "maange  मांगे\n",
            "maajin  माजिन\n",
            "maap  माप\n",
            "mamooli  मामूली\n",
            "maariya  मारिया\n",
            "maariyo  मारियो\n",
            "maalish  मालिश\n",
            "malish  मालिश\n",
            "maas  मास\n",
            "maason  मासों\n",
            "mitaao  मिटाओ\n",
            "mitao  मिटाओ\n",
            "mimicry  मिमिक्री\n",
            "mundva  मुंडवा\n",
            "mundwa  मुंडवा\n",
            "mukartey  मुकरते\n",
            "mukhyamantri  मुख्यमंत्री\n",
            "muhavare  मुहावरे\n",
            "muhavre  मुहावरे\n",
            "muth  मूठ\n",
            "mool  मूल\n",
            "mul  मूल\n",
            "moolchand  मूलचंद\n",
            "mulchand  मूलचंद\n",
            "moolon  मूलों\n",
            "mulon  मूलों\n",
            "mega  मेगा\n",
            "mejbaan  मेजबान\n",
            "mezbaan  मेजबान\n",
            "mezabani  मेजबानी\n",
            "mate  मेट\n",
            "met  मेट\n",
            "mates  मेट्स\n",
            "menu  मेनू\n",
            "mem  मेम\n",
            "melaanin  मेलानिन\n",
            "melanin  मेलानिन\n",
            "mainan  मैनन\n",
            "maine  मैने\n",
            "merraige  मैरेज\n",
            "maisore  मैसोर\n",
            "mogali  मोगली\n",
            "mowgli  मोगली\n",
            "motive  मोटिव\n",
            "mold  मोल्ड\n",
            "mohini  मोहिनी\n",
            "mohim  मोहिम\n",
            "maukon  मौकों\n",
            "mauma  मौमा\n",
            "mauryon  मौर्यों\n",
            "maulaviyon  मौलवियों\n",
            "maulviyon  मौलवियों\n",
            "maule  मौले\n",
            "mauley  मौले\n",
            "mausa  मौसा\n",
            "yaduwansh  यदुवंश\n",
            "yaatna  यातना\n",
            "yatraein  यात्राएं\n",
            "yatraen  यात्राएं\n",
            "yaariyan  यारियां\n",
            "yaari  यारी\n",
            "yugam  युग्म\n",
            "yojanaein  योजनाएं\n",
            "yojanaen  योजनाएं\n",
            "yojanaye  योजनाएं\n",
            "rangate  रंगते\n",
            "rangte  रंगते\n",
            "ranjkta  रंजकता\n",
            "rakhna  रखना\n",
            "raghupati  रघुपति\n",
            "runners  रनर्स\n",
            "rasad  रसद\n",
            "rahamat  रहमत\n",
            "rahmat  रहमत\n",
            "rhamat  रहमत\n",
            "rahenge  रहेंगे\n",
            "rahegi  रहेगी\n",
            "raak  राक\n",
            "raagon  रागों\n",
            "raaj  राज\n",
            "raj  राज\n",
            "rajkot  राजकोट\n",
            "rajtantra  राजतंत्र\n",
            "raji  राजी\n",
            "raajon  राजों\n",
            "ramkot  रामकोट\n",
            "ramkote  रामकोट\n",
            "rig  रिग\n",
            "ria  रिया\n",
            "riya  रिया\n",
            "risaney  रिसने\n",
            "reeta  रीता\n",
            "rukate  रुकते\n",
            "roopesh  रुपेश\n",
            "root  रूट\n",
            "route  रूट\n",
            "roots  रूट्स\n",
            "roopak  रूपक\n",
            "rupak  रूपक\n",
            "roopi  रूपी\n",
            "rental  रेंटल\n",
            "reja  रेजा\n",
            "reza  रेजा\n",
            "rate  रेट\n",
            "rese  रेस\n",
            "racer  रेसर\n",
            "racing  रेसिंग\n",
            "rojlin  रोजलिन\n",
            "rotation  रोटेशन\n",
            "roni  रोनी\n",
            "robin  रोबिन\n",
            "roya  रोया\n",
            "raubadaar  रौबदार\n",
            "raubdar  रौबदार\n",
            "langada  लंगड़ा\n",
            "langar  लंगर\n",
            "langaron  लंगरों\n",
            "langron  लंगरों\n",
            "lambi  लंबी\n",
            "lakeer  लकीर\n",
            "lakiron  लकीरों\n",
            "lakshy  लक्ष्य\n",
            "lakshyon  लक्ष्यों\n",
            "luxor  लक्सर\n",
            "lagega  लगेगा\n",
            "ladki  लड़की\n",
            "ladate  लड़ते\n",
            "ladte  लड़ते\n",
            "labdh  लब्ध\n",
            "lassi  लस्सी\n",
            "lahanaa  लहना\n",
            "lahna  लहना\n",
            "lehna  लहना\n",
            "lati  लाती\n",
            "layega  लायेगा\n",
            "lindan  लिंडन\n",
            "leaderon  लीडरों\n",
            "lumding  लुमडिंग\n",
            "lootmaar  लूटमार\n",
            "lega  लेगा\n",
            "legaa  लेगा\n",
            "letakar  लेटकर\n",
            "letkar  लेटकर\n",
            "label  लेबल\n",
            "lairy  लैरी\n",
            "locket  लॉकेट\n",
            "lochdar  लोचदार\n",
            "vansh  वंश\n",
            "wajood  वजूद\n",
            "varn  वर्ण\n",
            "vartamaan  वर्तमान\n",
            "vartaman  वर्तमान\n",
            "vartmaan  वर्तमान\n",
            "verbal  वर्बल\n",
            "valkan  वल्कन\n",
            "vash  वश\n",
            "vatar  वाटर\n",
            "vaadak  वादक\n",
            "vadak  वादक\n",
            "vaadiyon  वादियों\n",
            "vadiyon  वादियों\n",
            "wadiyon  वादियों\n",
            "vastvik  वास्तविक\n",
            "vaahikayen  वाहिकाएं\n",
            "vahikayen  वाहिकाएं\n",
            "wahikayan  वाहिकाएं\n",
            "vincent  विंसेंट\n",
            "vikalpon  विकल्पों\n",
            "wicket  विकेट\n",
            "vichaarakon  विचारकों\n",
            "vicharakon  विचारकों\n",
            "vigyan  विज्ञान\n",
            "vigyaani  विज्ञानी\n",
            "vigyani  विज्ञानी\n",
            "vigyaanon  विज्ञानों\n",
            "vigyanon  विज्ञानों\n",
            "vidyaayein  विद्याएं\n",
            "vidyaaon  विद्याओं\n",
            "vidhansabhaein  विधानसभाएं\n",
            "vidhansabhaen  विधानसभाएं\n",
            "vineet  विनीत\n",
            "vinit  विनीत\n",
            "vibha  विभा\n",
            "vibhajak  विभाजक\n",
            "virajati  विराजती\n",
            "virodh  विरोध\n",
            "visheshaadhikaar  विशेषाधिकार\n",
            "visheshadhikar  विशेषाधिकार\n",
            "veeresh  वीरेश\n",
            "vrutton  वृत्तों\n",
            "wedding  वेडिंग\n",
            "vediyon  वेदियों\n",
            "vedon  वेदों\n",
            "veronika  वेरोनिका\n",
            "weronika  वेरोनिका\n",
            "welwet  वेलवेट\n",
            "weldor  वेल्डर\n",
            "welding  वेल्डिंग\n",
            "wealth  वेल्थ\n",
            "westing  वेस्टिंग\n",
            "vaigyaanik  वैज्ञानिक\n",
            "vaigyaanikta  वैज्ञानिकता\n",
            "vaigyanikata  वैज्ञानिकता\n",
            "vaigyanikta  वैज्ञानिकता\n",
            "vaitravati  वैत्रवती\n",
            "vaishakhi  वैशाखी\n",
            "vyavadhan  व्यवधान\n",
            "vyavdhan  व्यवधान\n",
            "vyapak  व्यापक\n",
            "shanghai  शंघाई\n",
            "shanivar  शनिवार\n",
            "shaniwar  शनिवार\n",
            "shaharwasiyon  शहरवासियों\n",
            "shahrvasiyon  शहरवासियों\n",
            "shaantikaal  शांतिकाल\n",
            "shaakhaaen  शाखाएं\n",
            "shaavakon  शावकों\n",
            "shaasaka  शासक\n",
            "shasak  शासक\n",
            "shaasakon  शासकों\n",
            "shasakon  शासकों\n",
            "sheershakon  शीर्षकों\n",
            "shujaat  शुजात\n",
            "shujat  शुजात\n",
            "shubhankar  शुभंकर\n",
            "shumaali  शुमाली\n",
            "shumali  शुमाली\n",
            "shaili  शैली\n",
            "shaishav  शैशव\n",
            "shrikhand  श्रीखंड\n",
            "sankhyaein  संख्याएं\n",
            "sankhyaen  संख्याएं\n",
            "sankhyayen  संख्याएं\n",
            "sanchayan  संचयन\n",
            "sanchaari  संचारी\n",
            "sanjay  संजय\n",
            "sanjeevan  संजीवन\n",
            "sanjivan  संजीवन\n",
            "sandarbh  संदर्भ\n",
            "sandarbha  संदर्भ\n",
            "sandarbhon  संदर्भों\n",
            "sandli  संदली\n",
            "sambit  संबित\n",
            "sambhalane  संभालने\n",
            "sambhalne  संभालने\n",
            "sanyukt  संयुक्त\n",
            "sanyukta  संयुक्त\n",
            "sanyojan  संयोजन\n",
            "sanyojanon  संयोजनों\n",
            "sanyojnon  संयोजनों\n",
            "samvedikaran  संवेदीकरण\n",
            "sanvedikaran  संवेदीकरण\n",
            "sanshayon  संशयों\n",
            "sanskaar  संस्कार\n",
            "sanskar  संस्कार\n",
            "sanskaari  संस्कारी\n",
            "sanskurt  संस्कृत\n",
            "sanskritiyon  संस्कृतियों\n",
            "sanskrutiyon  संस्कृतियों\n",
            "sansthit  संस्थित\n",
            "sakra  सकरा\n",
            "sate  सते\n",
            "sapane  सपने\n",
            "sapne  सपने\n",
            "sapney  सपने\n",
            "saparivaar  सपरिवार\n",
            "sapariwar  सपरिवार\n",
            "saptari  सप्तरी\n",
            "sabhaein  सभाएं\n",
            "sabhaen  सभाएं\n",
            "sabhasadon  सभासदों\n",
            "sabhyata  सभ्यता\n",
            "samjhungaa  समझूंगा\n",
            "saman  समन\n",
            "samanvayaka  समन्वयक\n",
            "samanwayaka  समन्वयक\n",
            "samnvyak  समन्वयक\n",
            "samadhiyon  समाधियों\n",
            "samitiyan  समितियां\n",
            "sametati  समेटती\n",
            "sarka  सरका\n",
            "sarkaa  सरका\n",
            "sarkarein  सरकारें\n",
            "sarkaaron  सरकारों\n",
            "sarkaron  सरकारों\n",
            "sargi  सरगी\n",
            "sarsa  सरसा\n",
            "sarsaa  सरसा\n",
            "saleeb  सलीब\n",
            "salib  सलीब\n",
            "sahrawat  सहरावत\n",
            "sahuliyat  सहूलियत\n",
            "sahuuliyata  सहूलियत\n",
            "sagarika  सागरिका\n",
            "sagrika  सागरिका\n",
            "saajan  साजन\n",
            "sathiya  साथिया\n",
            "saabah  साबाह\n",
            "sabah  साबाह\n",
            "singaar  सिंगार\n",
            "singar  सिंगार\n",
            "sikud  सिकुड़\n",
            "sinabang  सिनाबंग\n",
            "sim  सिम\n",
            "simi  सिमी\n",
            "siyasi  सियासी\n",
            "siyah  सियाह\n",
            "siyaha  सियाह\n",
            "sivan  सिवान\n",
            "siwan  सिवान\n",
            "seem  सीम\n",
            "sugamata  सुगमता\n",
            "sujen  सुजेन\n",
            "sujhaati  सुझाती\n",
            "sujhati  सुझाती\n",
            "sutali  सुतली\n",
            "sdueep  सुदीप\n",
            "sudip  सुदीप\n",
            "sunaaron  सुनारों\n",
            "sunaron  सुनारों\n",
            "sunaseer  सुनासीर\n",
            "sunasir  सुनासीर\n",
            "sunega  सुनेगा\n",
            "suneyga  सुनेगा\n",
            "sulkhan  सुलखान\n",
            "suljhai  सुलझाई\n",
            "sulaati  सुलाती\n",
            "suhagin  सुहागिन\n",
            "soochak  सूचक\n",
            "sensing  सेंसिंग\n",
            "seti  सेटी\n",
            "seto  सेटो\n",
            "semin  सेमिन\n",
            "sevakon  सेवकों\n",
            "sevkon  सेवकों\n",
            "soma  सोमा\n",
            "somi  सोमी\n",
            "soy  सोय\n",
            "soya  सोया\n",
            "soharai  सोहराई\n",
            "sohrai  सोहराई\n",
            "saunpane  सौंपने\n",
            "sonpne  सौंपने\n",
            "ski  स्की\n",
            "skhalan  स्खलन\n",
            "stop  स्टॉप\n",
            "stops  स्टॉप्स\n",
            "stone  स्टोन\n",
            "straws  स्ट्रास\n",
            "standhaari  स्तनधारी\n",
            "standhari  स्तनधारी\n",
            "sthool  स्थूल\n",
            "sthoolta  स्थूलता\n",
            "sneakers  स्नीकर्स\n",
            "spear  स्पीयर\n",
            "spell  स्पेल\n",
            "spelar  स्पेलर\n",
            "speller  स्पेलर\n",
            "sphoorti  स्फूर्ति\n",
            "swa  स्वा\n",
            "handiya  हंडिया\n",
            "handiyaa  हंडिया\n",
            "hundiya  हंडिया\n",
            "hakikat  हकीकत\n",
            "hathiya  हथिया\n",
            "hamala  हमला\n",
            "hamla  हमला\n",
            "hamlavar  हमलावर\n",
            "harpaal  हरपाल\n",
            "harpal  हरपाल\n",
            "haraval  हरावल\n",
            "harawal  हरावल\n",
            "harijan  हरिजन\n",
            "hariyali  हरियाली\n",
            "hasino  हसीनों\n",
            "hasinon  हसीनों\n",
            "hadauti  हाड़ौती\n",
            "haani  हानी\n",
            "hingis  हिंगिस\n",
            "hijada  हिजड़ा\n",
            "hippo  हिप्पो\n",
            "himali  हिमाली\n",
            "hilane  हिलने\n",
            "hilne  हिलने\n",
            "hisham  हिशाम\n",
            "hemwati  हेमवती\n",
            "holding  होल्डिंग\n",
            "0.2598844957796535\n"
          ]
        }
      ],
      "source": [
        "def prepareTestData(test_path):\n",
        "    lines = open(test_path,'rt', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    lang_in_texts = []\n",
        "    lang_out_texts = []\n",
        "\n",
        "    for line in lines:\n",
        "        lang_out_text, lang_in_text, _ = line.strip('\\n').split(\"\\t\")\n",
        "        lang_in_texts.append(lang_in_text)\n",
        "        lang_out_texts.append(lang_out_text)\n",
        "\n",
        "    test_data={}\n",
        "\n",
        "    test_data.update({'input':lang_in_texts})\n",
        "    test_data.update({'target':lang_out_texts})\n",
        "    return test_data\n",
        "\n",
        "def test_validate(encoder, decoder, cell_type,outF, attention = 0):\n",
        "    accuracy=0\n",
        "    for index in range( len(test_data['input'])):\n",
        "        pair = tensorPairs(test_data['input'][index], test_data['target'][index])\n",
        "        output_word = evaluate(encoder, decoder, cell_type, pair[0],attention = attention)\n",
        "\n",
        "        if (output_word == test_data['target'][index]):\n",
        "            accuracy+=1\n",
        "\n",
        "            l = f\"{test_data['input'][index]}  {output_word}\"\n",
        "            outF.write(l)\n",
        "            outF.write(\"\\n\")\n",
        "            print(l)\n",
        "    print(accuracy/(len(test_data['target'])))\n",
        "\n",
        "\n",
        "\n",
        "outF = open(\"best_vanilla_pred.txt\", \"w\")\n",
        "\n",
        "test_data = prepareTestData('dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv')\n",
        "\n",
        "test_validate(encoder1,decoder1,'lstm',outF,attention=0)\n",
        "outF.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3bEL_NhevpO"
      },
      "outputs": [],
      "source": [
        "evaluateRandomly(encoder1, decoder1, 'lstm', n=100,attention=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfibQ8pXeyJ_"
      },
      "outputs": [],
      "source": [
        "validate(encoder1, decoder1, 'lstm', 0.1,attention=0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}